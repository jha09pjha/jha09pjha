{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMa9WqkS8DMNJ6nnXvvQpL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jha09pjha/jha09pjha/blob/main/attention_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbnfxT-k9ou7",
        "outputId": "7c643205-3f00-4d77-bd54-7fa062972967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  hidden_size: 128\n",
            "  num_attention_heads: 16\n",
            "  num_key_value_heads: 4\n",
            "  head_dim: 8\n",
            "\n",
            "Sample Input Shapes:\n",
            "  hidden_states: torch.Size([2, 10, 128])\n",
            "  position_ids: torch.Size([2, 10])\n",
            "  attention_mask: torch.Size([2, 1, 10, 10])\n",
            "Projected Shapes:\n",
            "  query_states: torch.Size([2, 16, 10, 8])\n",
            "  key_states: torch.Size([2, 4, 10, 8])\n",
            "  value_states: torch.Size([2, 4, 10, 8])\n",
            "\n",
            "Num Key/Value Groups (Q heads per K/V head): 4\n",
            "Calculated freqs_cis shape: torch.Size([256, 8])\n",
            "\n",
            "Shapes after RoPE:\n",
            "  query_states_rope: torch.Size([2, 16, 10, 8])\n",
            "  key_states_rope: torch.Size([2, 4, 10, 8])\n",
            "\n",
            "Applied QK Norm\n",
            "\n",
            "Shapes before attention score calculation:\n",
            "  query_states_final: torch.Size([2, 16, 10, 8])\n",
            "  key_states_final: torch.Size([2, 4, 10, 8])\n",
            "\n",
            "Shapes after repeating K/V for GQA:\n",
            "  key_states_repeated: torch.Size([2, 16, 10, 8])\n",
            "  value_states_repeated: torch.Size([2, 16, 10, 8])\n",
            "\n",
            "Applying attention mask with shape: torch.Size([2, 1, 10, 10])\n",
            "\n",
            "Attention Calculation Shapes:\n",
            "  attn_weights (raw scores): torch.Size([2, 16, 10, 10])\n",
            "  attn_weights (after softmax): torch.Size([2, 16, 10, 10])\n",
            "  attn_output: torch.Size([2, 16, 10, 8])\n",
            "\n",
            "Final Output Shapes:\n",
            "  attn_output (reshaped): torch.Size([2, 10, 128])\n",
            "  final_attn_output: torch.Size([2, 10, 128])\n",
            "\n",
            "Output shape from simplified module: torch.Size([2, 10, 128])\n",
            "Attention weights shape from simplified module: torch.Size([2, 16, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "# Model configuration parameters\n",
        "hidden_size = 128\n",
        "num_attention_heads = 16\n",
        "num_key_value_heads = 4\n",
        "head_dim = hidden_size // num_attention_heads\n",
        "max_position_embeddings = 256\n",
        "rope_theta = 10000.0\n",
        "rms_norm_eps = 1e-5\n",
        "attention_bias = False\n",
        "attention_dropout = 0.0\n",
        "use_qk_norm = True\n",
        "\n",
        "# Sample input tensor shapes\n",
        "batch_size = 2\n",
        "sequence_length = 10\n",
        "hidden_states = torch.randn(batch_size, sequence_length, hidden_size)\n",
        "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
        "# Causal attention mask\n",
        "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
        "attention_mask = attention_mask.unsqueeze(0).unsqueeze(0)\n",
        "attention_mask = attention_mask.expand(batch_size, 1, -1, -1)\n",
        "\n",
        "# Print configuration and input shapes\n",
        "print(\"Configuration:\")\n",
        "print(f\"  hidden_size: {hidden_size}\")\n",
        "print(f\"  num_attention_heads: {num_attention_heads}\")\n",
        "print(f\"  num_key_value_heads: {num_key_value_heads}\")\n",
        "print(f\"  head_dim: {head_dim}\")\n",
        "\n",
        "print(\"\\nSample Input Shapes:\")\n",
        "print(f\"  hidden_states: {hidden_states.shape}\")\n",
        "print(f\"  position_ids: {position_ids.shape}\")\n",
        "print(f\"  attention_mask: {attention_mask.shape}\")\n",
        "\n",
        "# Linear projections for Q, K, V, and Output\n",
        "q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias=attention_bias)\n",
        "k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
        "v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
        "o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias=attention_bias)\n",
        "\n",
        "# Project input hidden states to Q, K, V\n",
        "query_states = q_proj(hidden_states)\n",
        "key_states = k_proj(hidden_states)\n",
        "value_states = v_proj(hidden_states)\n",
        "\n",
        "# Reshape and transpose Q, K, V for multi-head attention\n",
        "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1, 2)\n",
        "key_states = key_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
        "value_states = value_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "# Print projected shapes\n",
        "print(\"Projected Shapes:\")\n",
        "print(f\"  query_states: {query_states.shape}\")\n",
        "print(f\"  key_states: {key_states.shape}\")\n",
        "print(f\"  value_states: {value_states.shape}\")\n",
        "\n",
        "# Calculate the number of key/value groups for Grouped Query Attention (GQA)\n",
        "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
        "print(f\"\\nNum Key/Value Groups (Q heads per K/V head): {num_key_value_groups}\")\n",
        "\n",
        "# Function to calculate rotary embedding frequencies\n",
        "def simple_rope_calculation(dim, max_seq_len, base=10000.0, device=None):\n",
        "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
        "    t = torch.arange(max_seq_len, device=device).type_as(inv_freq)\n",
        "    freqs = new_func(inv_freq, t)\n",
        "    emb = torch.cat((freqs, freqs), dim=-1)\n",
        "    freqs_cos = emb.cos()\n",
        "    freqs_sin = emb.sin()\n",
        "    freqs_cis = torch.complex(freqs_cos, freqs_sin)\n",
        "    return freqs_cis\n",
        "\n",
        "def new_func(inv_freq, t):\n",
        "    freqs = torch.outer(t, inv_freq)\n",
        "    return freqs\n",
        "\n",
        "# Function to apply rotary embeddings to query and key states\n",
        "def apply_rotary_emb_torch(\n",
        "    xq: torch.Tensor,\n",
        "    xk: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    freqs_cis = freqs_cis.to(xq.device)\n",
        "\n",
        "    freqs_cis = freqs_cis[position_ids]\n",
        "\n",
        "    freqs_cis = freqs_cis[:, None, :, :]\n",
        "\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "\n",
        "    freqs_cis_broadcast = freqs_cis[..., :xq_.shape[-1]]\n",
        "\n",
        "    rotated_xq = xq_ * freqs_cis_broadcast\n",
        "    rotated_xk = xk_ * freqs_cis_broadcast\n",
        "\n",
        "    xq_out = torch.view_as_real(rotated_xq).flatten(3)\n",
        "    xk_out = torch.view_as_real(rotated_xk).flatten(3)\n",
        "\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "# Calculate and apply RoPE\n",
        "freqs_cis = simple_rope_calculation(head_dim, max_position_embeddings, base=rope_theta, device=hidden_states.device)\n",
        "print(f\"Calculated freqs_cis shape: {freqs_cis.shape}\")\n",
        "\n",
        "query_states_rope, key_states_rope = apply_rotary_emb_torch(query_states, key_states, freqs_cis)\n",
        "\n",
        "print(\"\\nShapes after RoPE:\")\n",
        "print(f\"  query_states_rope: {query_states_rope.shape}\")\n",
        "print(f\"  key_states_rope: {key_states_rope.shape}\")\n",
        "\n",
        "# Simple L2 Normalization class\n",
        "class SimpleL2Norm(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "# Apply QK Normalization if enabled\n",
        "if use_qk_norm:\n",
        "    qk_norm = SimpleL2Norm()\n",
        "    query_states_final = qk_norm(query_states_rope)\n",
        "    key_states_final = qk_norm(key_states_rope)\n",
        "    print(\"\\nApplied QK Norm\")\n",
        "else:\n",
        "    query_states_final = query_states_rope\n",
        "    key_states_final = key_states_rope\n",
        "    print(\"\\nSkipped QK Norm\")\n",
        "\n",
        "print(\"\\nShapes before attention score calculation:\")\n",
        "print(f\"  query_states_final: {query_states_final.shape}\")\n",
        "print(f\"  key_states_final: {key_states_final.shape}\")\n",
        "\n",
        "# Function to repeat key/value states for GQA\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "# Repeat key and value states for GQA\n",
        "key_states_repeated = repeat_kv(key_states_final, num_key_value_groups)\n",
        "value_states_repeated = repeat_kv(value_states, num_key_value_groups)\n",
        "\n",
        "print(\"\\nShapes after repeating K/V for GQA:\")\n",
        "print(f\"  key_states_repeated: {key_states_repeated.shape}\")\n",
        "print(f\"  value_states_repeated: {value_states_repeated.shape}\")\n",
        "\n",
        "# Calculate attention weights (dot product of Q and K)\n",
        "attn_weights = torch.matmul(query_states_final, key_states_repeated.transpose(2, 3))\n",
        "\n",
        "# Apply scaling factor\n",
        "scaling_factor = 1.0 / math.sqrt(head_dim)\n",
        "attn_weights = attn_weights * scaling_factor\n",
        "\n",
        "# Apply attention mask\n",
        "if attention_mask is not None:\n",
        "    print(f\"\\nApplying attention mask with shape: {attention_mask.shape}\")\n",
        "    causal_mask = attention_mask[:, :, :, :key_states_repeated.shape[-2]]\n",
        "    attn_weights = attn_weights + causal_mask\n",
        "else:\n",
        "     print(\"\\nNo attention mask applied.\")\n",
        "\n",
        "# Apply softmax to get attention probabilities\n",
        "attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n",
        "\n",
        "# Calculate attention output (weighted sum of V)\n",
        "attn_output = torch.matmul(attn_weights, value_states_repeated)\n",
        "\n",
        "print(\"\\nAttention Calculation Shapes:\")\n",
        "print(f\"  attn_weights (raw scores): {attn_weights.shape}\")\n",
        "print(f\"  attn_weights (after softmax): {attn_weights.shape}\")\n",
        "print(f\"  attn_output: {attn_output.shape}\")\n",
        "\n",
        "# Reshape attention output back to original dimensions\n",
        "attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "attn_output = attn_output.view(batch_size, sequence_length, hidden_size)\n",
        "\n",
        "# Apply output projection\n",
        "final_attn_output = o_proj(attn_output)\n",
        "\n",
        "print(\"\\nFinal Output Shapes:\")\n",
        "print(f\"  attn_output (reshaped): {attn_output.shape}\")\n",
        "print(f\"  final_attn_output: {final_attn_output.shape}\")\n",
        "\n",
        "# Simple Feed-Forward Network class\n",
        "class SimpleFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_size, intermediate_size):\n",
        "        super().__init__()\n",
        "        # Linear layer for gate projection\n",
        "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
        "        # Linear layer for up projection\n",
        "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
        "        # Linear layer for down projection\n",
        "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply SiLU activation to gate projection, element-wise multiply with up projection, and then apply down projection\n",
        "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "# Simplified Llama-like Attention module with Feed-Forward Network\n",
        "class SimplifiedLlama4Attention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config['hidden_size']\n",
        "        self.num_attention_heads = config['num_attention_heads']\n",
        "        self.num_key_value_heads = config['num_key_value_heads']\n",
        "        self.head_dim = self.hidden_size // self.num_attention_heads\n",
        "        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config['max_position_embeddings']\n",
        "        self.rope_theta = config['rope_theta']\n",
        "        self.attention_bias = config['attention_bias']\n",
        "        self.use_qk_norm = config['use_qk_norm']\n",
        "        self.intermediate_size = config.get('intermediate_size', 4 * self.hidden_size)\n",
        "\n",
        "        if (self.head_dim * self.num_attention_heads) != self.hidden_size:\n",
        "            raise ValueError(\"hidden_size must be divisible by num_attention_heads\")\n",
        "\n",
        "        # Linear projections\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=self.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, self.hidden_size, bias=self.attention_bias)\n",
        "\n",
        "        # Rotary embeddings\n",
        "        self.freqs_cis = simple_rope_calculation(self.head_dim, self.max_position_embeddings, base=self.rope_theta)\n",
        "\n",
        "        # Optional QK Normalization\n",
        "        if self.use_qk_norm:\n",
        "             self.qk_norm = SimpleL2Norm()\n",
        "\n",
        "        # Feed-Forward Network layer\n",
        "        self.feed_forward = SimpleFeedForward(self.hidden_size, self.intermediate_size)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, position_ids):\n",
        "        batch_size, sequence_length, _ = hidden_states.shape\n",
        "\n",
        "        # Project input hidden states\n",
        "        query_states = self.q_proj(hidden_states)\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # Reshape and transpose for multi-head attention\n",
        "        query_states = query_states.view(batch_size, sequence_length, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(batch_size, sequence_length, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(batch_size, sequence_length, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Apply rotary embeddings\n",
        "        current_freqs_cis = self.freqs_cis.to(hidden_states.device)\n",
        "        query_states_rope, key_states_rope = apply_rotary_emb_torch(query_states, key_states, current_freqs_cis)\n",
        "\n",
        "        # Apply QK Normalization\n",
        "        if self.use_qk_norm:\n",
        "             query_states_final = self.qk_norm(query_states_rope)\n",
        "             key_states_final = self.qk_norm(key_states_rope)\n",
        "        else:\n",
        "            query_states_final = query_states_rope\n",
        "            key_states_final = key_states_rope\n",
        "\n",
        "        # Repeat key/value states for GQA\n",
        "        key_states_repeated = repeat_kv(key_states_final, self.num_key_value_groups)\n",
        "        value_states_repeated = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attn_weights = torch.matmul(query_states_final, key_states_repeated.transpose(2, 3))\n",
        "        scaling_factor = 1.0 / math.sqrt(self.head_dim)\n",
        "        attn_weights = attn_weights * scaling_factor\n",
        "\n",
        "        # Apply attention mask\n",
        "        if attention_mask is not None:\n",
        "            causal_mask = attention_mask[:, :, :, :key_states_repeated.shape[-2]]\n",
        "            attn_weights = attn_weights + causal_mask\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value_states_repeated)\n",
        "\n",
        "        # Reshape attention output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, sequence_length, self.hidden_size)\n",
        "\n",
        "        # Apply output projection\n",
        "        final_attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        # Pass the output through the feed-forward network\n",
        "        output_with_ffn = self.feed_forward(final_attn_output)\n",
        "\n",
        "        return output_with_ffn, attn_weights\n",
        "\n",
        "# Configuration dictionary\n",
        "config_dict = {\n",
        "    'hidden_size': hidden_size,\n",
        "    'num_attention_heads': num_attention_heads,\n",
        "    'num_key_value_heads': num_key_value_heads,\n",
        "    'max_position_embeddings': max_position_embeddings,\n",
        "    'rope_theta': rope_theta,\n",
        "    'attention_bias': attention_bias,\n",
        "    'use_qk_norm': use_qk_norm,\n",
        "    'intermediate_size': 4 * hidden_size\n",
        "}\n",
        "\n",
        "# Instantiate and run the simplified attention module with FFN\n",
        "simplified_attn_module = SimplifiedLlama4Attention(config_dict)\n",
        "\n",
        "final_output_simplified, final_weights_simplified = simplified_attn_module(hidden_states, attention_mask, position_ids)\n",
        "\n",
        "# Print output shapes\n",
        "print(\"\\nOutput shape from simplified module:\", final_output_simplified.shape)\n",
        "print(\"Attention weights shape from simplified module:\", final_weights_simplified.shape)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9600d560"
      },
      "source": [
        "class SimpleFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feed-forward network with two linear layers and a SiLU activation.\n",
        "    This is a common component in transformer models.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, intermediate_size):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
        "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
        "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
        "        self.act_fn = F.silu\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First linear transformation and SiLU activation\n",
        "        gate = self.act_fn(self.gate_proj(x))\n",
        "        # Second linear transformation\n",
        "        up = self.up_proj(x)\n",
        "        # Element-wise multiplication of the two branches\n",
        "        intermediate = gate * up\n",
        "        # Final linear transformation\n",
        "        down = self.down_proj(intermediate)\n",
        "        return down"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}